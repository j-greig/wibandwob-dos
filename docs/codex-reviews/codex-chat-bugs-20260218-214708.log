## Bug 1: `wibandwob.prompt.md not found - using fallback prompt`

**What I found**
- `app/wibwob_engine.cpp` / `.h` **do not** look for `wibandwob.prompt.md` at all.
- The startup message is emitted from `TWibWobWindow::ensureEngineInitialized()` in `app/wibwob_view.cpp` when it can’t open the prompt file from a **fixed list of CWD-relative paths**.
- `app/llm/providers/claude_code_provider.cpp` also had prompt-file checks that were effectively CWD-relative (and inconsistent between non-streaming vs streaming).
- `wibandwob.prompt.md` is **not present anywhere in this repo** in the current workspace (so the message is currently expected). The fix below makes it get found if you place it in repo root or `app/` even when launching from `build/app/`.

**Fix**
- Added `app/llm/base/path_search.h` and switched prompt/config/script lookups to **search upward from CWD** (so running from repo root *or* from `build/app/` works).

## Bug 2: App freezes waiting for LLM response

**What I found**
- When the engine selects `anthropic_api` (it auto-overrides to `anthropic_api` if `ANTHROPIC_API_KEY` is set), the call path is:
  - `TWibWobWindow::fallbackToRegularQuery()` → `WibWobEngine::sendQuery()` → `AnthropicAPIProvider::sendQuery()`
- `AnthropicAPIProvider::sendQuery()` was **synchronous**: it ran `curl` and did a blocking `fgets` loop on the **Turbo Vision main thread**, freezing the entire event loop.

**Fix**
- Converted `AnthropicAPIProvider` to **non-blocking**:
  - `sendQuery()` starts `curl` with a non-blocking pipe and returns immediately.
  - `poll()` drains output and fires the callback on completion.
- Ensured the chat window reliably polls providers by adding a dedicated 50ms timer in `TWibWobWindow` that calls `engine->poll()`.

## Verification
- `cmake --build build --target test_pattern` succeeds.
- Full `cmake --build build` fails due to pre-existing undefined symbols in test targets (`api_chat_receive`), unrelated to these changes.

---

# Complete replacement code (all functions changed)

### `app/wibwob_view.cpp` — `TWibWobWindow` ctor/dtor/event/prompt loading

```cpp
TWibWobWindow::TWibWobWindow(const TRect& bounds, const std::string& title)
    : TWindow(bounds, title.c_str(), wnNoNumber)
    , TWindowInit(&TWibWobWindow::initFrame)
    , baseTitle(title)
{
    options |= ofTileable;
    growMode = gfGrowHiX | gfGrowHiY;
    eventMask |= evKeyDown | evBroadcast;

    TRect client = getExtent();
    client.grow(-1, -1);

    // Split: message pane on top, input at bottom
    TRect msgPaneRect = client;
    msgPaneRect.b.y -= 2;  // Leave 2 rows for input

    messagePane = new TGroup(msgPaneRect);
    messagePane->growMode = gfGrowHiX | gfGrowHiY;
    insert(messagePane);

    // Input view: 2 lines at bottom (status + input)
    TRect inputRect = client;
    inputRect.a.y = inputRect.b.y - 2;
    inputView = new TWibWobInputView(inputRect);
    inputView->growMode = gfGrowHiX | gfGrowLoY | gfGrowHiY;
    insert(inputView);

    // Inside the message pane: dedicated scrollbar and scroller
    TRect paneBounds = messagePane->getExtent();
    paneBounds.move(-paneBounds.a.x, -paneBounds.a.y);

    TRect sbRect = paneBounds;
    sbRect.a.x = sbRect.b.x - 1;
    vScrollBar = new TScrollBar(sbRect);
    vScrollBar->options |= ofPostProcess; // Allow keyboard to reach the bar (sbHandleKeyboard equivalent)
    vScrollBar->growMode = gfGrowLoX | gfGrowHiX | gfGrowHiY;
    messagePane->insert(vScrollBar);

    TRect msgRect = paneBounds;
    msgRect.b.x -= 1;
    messageView = new TWibWobMessageView(msgRect, nullptr, vScrollBar);
    messageView->growMode = gfGrowHiX | gfGrowHiY;
    messagePane->insert(messageView);

    // Set up input callback
    inputView->onSubmit = [this](const std::string& input) {
        processUserInput(input);
    };

    // Focus the input view by default
    inputView->select();
}

TWibWobWindow::~TWibWobWindow() {
    if (pollTimerId) {
        killTimer(pollTimerId);
        pollTimerId = nullptr;
    }
    delete engine;
}

void TWibWobWindow::handleEvent(TEvent& event) {
    TWindow::handleEvent(event);

    // Handle ESC for cancel
    if (event.what == evKeyDown && event.keyDown.keyCode == kbEsc) {
        ensureEngineInitialized();
        if (engine && engine->isBusy()) {
            engine->cancel();
            inputView->setStatus("Request cancelled - Type a message and press Enter");
            inputView->setInputEnabled(true);
            inputView->stopSpinner();
            clearEvent(event);
        }
    }

    // Poll engine on timer broadcasts
    if (event.what == evBroadcast && event.message.command == cmTimerExpired) {
        if (event.message.infoPtr == pollTimerId && engineInitialized && engine) {
            engine->poll();
        }
    }
}

void TWibWobWindow::ensureEngineInitialized() {
    if (!engineInitialized) {
        // Initialize logging first
        if (logFilePath.empty()) {
            initializeLogging();
        }

        engine = new WibWobEngine();

        // Ensure we poll providers frequently (some providers rely on poll() to read subprocess output).
        if (!pollTimerId) {
            pollTimerId = setTimer(50, 50);
        }

        // Inject runtime API key if set (e.g. from Tools > API Key dialog)
        {
            extern std::string getAppRuntimeApiKey();
            std::string rtKey = getAppRuntimeApiKey();
            if (!rtKey.empty()) {
                engine->setApiKey(rtKey);
                fprintf(stderr, "DEBUG: Injected runtime API key into chat engine\n");
            }
        }

        // Load system prompt from file.
        // Important: app is commonly launched from either repo root OR build/app; search upward.
        const std::vector<std::string> promptCandidates = {
            "modules-private/wibwob-prompts/wibandwob.prompt.md",
            "modules/wibwob-prompts/wibandwob.prompt.md",
            "wibandwob.prompt.md",
            "app/wibandwob.prompt.md",
            "test-tui/wibandwob.prompt.md",
            "app/test-tui/wibandwob.prompt.md",
        };

        const std::string loadedPath = ww_find_first_existing_upwards(promptCandidates, 6);
        std::ifstream promptFile;
        if (!loadedPath.empty()) {
            promptFile.open(loadedPath);
        }

        if (promptFile.is_open() && !loadedPath.empty()) {
            std::string customPrompt((std::istreambuf_iterator<char>(promptFile)),
                                   std::istreambuf_iterator<char>());
            promptFile.close();

            engine->setSystemPrompt(customPrompt);
            messageView->addMessage("System", "Step into WibWobWorld, human.");
            logMessage("System", "Loaded custom prompt from " + loadedPath);
        } else {
            engine->setSystemPrompt(
                "You are wib&wob, a dual-minded artist/scientist AI assistant integrated into a Turbo Vision TUI application. "
                "Respond as both Wib (chaotic, artistic) and Wob (precise, scientific). "
                "Help with TVision framework, C++ development, and creative projects. "
                "Use British English and maintain your distinctive personalities."
            );
            messageView->addMessage("Wib", "Wotcher! I'm wib&wob, your AI assistant for this TVision app. (Note: wibandwob.prompt.md not found - using fallback prompt)");
        }

        inputView->setStatus("Ready - Type a message and press Enter");
        logMessage("System", "Chat engine initialized (provider loads on first send)");

        engineInitialized = true;
    }
}
```

### `app/wibwob_engine.cpp` — config file path resolution

```cpp
void WibWobEngine::loadConfiguration() {
    config = std::make_unique<LLMConfig>();
    
    // Config file path - app is commonly launched from either repo root OR build/app.
    const std::vector<std::string> cfgCandidates = {
        "app/llm/config/llm_config.json",
        "llm/config/llm_config.json"
    };
    bool loadResult = false;
    std::string usedPath;
    usedPath = ww_find_first_existing_upwards(cfgCandidates, 6);
    if (!usedPath.empty()) {
        loadResult = config->loadFromFile(usedPath);
    }

    fprintf(stderr, "DEBUG: Config file load result: %s%s\n",
            loadResult ? "SUCCESS " : "FAILED",
            loadResult ? ("(" + usedPath + ")").c_str() : "");

    // Pick desired provider based on env/config
    // Only override if ANTHROPIC_API_KEY is set and anthropic_api is configured
    auto hasAnthropicKey = []() -> bool {
        const char* v = std::getenv("ANTHROPIC_API_KEY");
        return v && *v;
    };
    std::string desiredProvider = config->getActiveProvider();
    fprintf(stderr, "DEBUG: Config activeProvider: %s\n", desiredProvider.c_str());

    // Only auto-switch to anthropic if key is present - otherwise respect config
    if (hasAnthropicKey() && config->hasProvider("anthropic_api")) {
        desiredProvider = "anthropic_api";
        fprintf(stderr, "DEBUG: Overriding to anthropic_api (API key present)\n");
    }
    // Only fall back to claude_code if no provider configured
    else if (desiredProvider.empty() && config->hasProvider("claude_code")) {
        desiredProvider = "claude_code";
        fprintf(stderr, "DEBUG: Falling back to claude_code (no provider configured)\n");
    }
    if (!desiredProvider.empty())
        config->setActiveProvider(desiredProvider);

    if (loadResult) {
        fprintf(stderr, "DEBUG: Config loaded successfully, active provider: %s\n", desiredProvider.c_str());
    } else {
        // Config file missing or invalid - create default but DON'T overwrite existing file
        fprintf(stderr, "ERROR: Failed to load llm/config/llm_config.json\n");
        fprintf(stderr, "DEBUG: Using default config with activeProvider: %s\n", desiredProvider.c_str());
        
        // Check validation errors
        auto errors = config->getValidationErrors();
        for (const auto& error : errors) {
            fprintf(stderr, "Config error: %s\n", error.c_str());
        }
        std::string defaultJson = LLMConfig::getDefaultConfigJson();
        config->loadFromString(defaultJson);
        
        // Only save if file doesn't exist at all
        FILE* check = fopen("llm/config/llm_config.json", "r");
        if (!check) {
            config->saveToFile("llm/config/llm_config.json");
        } else {
            fclose(check);
        }
        
        // Try the default active provider
        std::string defaultProvider = desiredProvider.empty() ? config->getActiveProvider() : desiredProvider;
        if (!initializeProvider(defaultProvider)) {
            // If default provider fails, try any available provider
            auto availableProviders = config->getAvailableProviders();
            for (const auto& provider : availableProviders) {
                if (initializeProvider(provider)) {
                    fprintf(stderr, "Fallback: Using provider '%s' instead of '%s'\n", provider.c_str(), defaultProvider.c_str());
                    break;
                }
            }
        }
    }

    // Initialize desired provider with fallback sequence: desired first, then the rest.
    std::vector<std::string> candidates;
    if (!desiredProvider.empty())
        candidates.push_back(desiredProvider);
    auto available = config->getAvailableProviders();
    for (const auto& p : available) {
        if (std::find(candidates.begin(), candidates.end(), p) == candidates.end())
            candidates.push_back(p);
    }
    for (const auto& name : candidates) {
        if (initializeProvider(name)) {
            fprintf(stderr, "DEBUG: Successfully initialized provider: %s\n", name.c_str());
            break;
        } else {
            fprintf(stderr, "ERROR: Failed to initialize provider: %s\n", name.c_str());
        }
    }
}
```

### `app/llm/providers/claude_code_provider.cpp` — prompt file resolution + streaming prompt support

```cpp
std::string ClaudeCodeProvider::buildClaudeCommand(const LLMRequest& request) const {
    std::ostringstream cmd;

    cmd << claudePath;

    // Add configured args (they already include -p, --mcp-config, etc.)
    for (const std::string& arg : commandArgs) {
        // Skip --output-format if already in args (we'll add it explicitly)
        if (arg.find("--output-format") != std::string::npos) continue;
        cmd << " " << arg;
    }

    // Always ensure JSON output
    cmd << " --output-format json";

    // Session management: use --resume with session ID if available
    if (!currentSessionId.empty()) {
        cmd << " --resume " << currentSessionId;
        fprintf(stderr, "DEBUG: Using session resume with ID: %s\n", currentSessionId.c_str());
    } else {
        fprintf(stderr, "DEBUG: Starting new session (no session ID)\n");
    }

    // System prompt: prefer a file if we can find one (works from repo root or build/app).
    const std::string promptFilePath = ww_find_first_existing_upwards({
        "wibandwob.prompt.md",
        "app/wibandwob.prompt.md",
        "modules-private/wibwob-prompts/wibandwob.prompt.md",
        "modules/wibwob-prompts/wibandwob.prompt.md",
        "test-tui/wibandwob.prompt.md",
        "app/test-tui/wibandwob.prompt.md",
    }, 6);

    if (!promptFilePath.empty()) {
        cmd << " --system-prompt-file " << promptFilePath;
    } else if (!request.system_prompt.empty()) {
        // Fallback to inline system prompt
        cmd << " --append-system-prompt \"";
        for (char c : request.system_prompt) {
            if (c == '"' || c == '\\' || c == '$' || c == '`') {
                cmd << '\\';
            }
            cmd << c;
        }
        cmd << "\"";
    }

    // Escape the query for shell
    cmd << " \"";
    for (char c : request.message) {
        if (c == '"' || c == '\\' || c == '$' || c == '`') {
            cmd << '\\';
        }
        cmd << c;
    }
    cmd << "\"";

    cmd << " 2>&1";  // Capture stderr too

    fprintf(stderr, "DEBUG: Claude command: %s\n", cmd.str().c_str());

    return cmd.str();
}
```

```cpp
bool ClaudeCodeProvider::sendStreamingQuery(const std::string& query, StreamingCallback streamCallback,
                                            const std::string& systemPrompt) {
    if (busy || query.empty()) {
        return false;
    }

    clearError();
    streamingMode = true;
    streamingActive = true;
    activeStreamCallback = streamCallback;
    lineBuffer.clear();

    // Build streaming command
    std::ostringstream cmd;
    cmd << claudePath;

    // Add configured args
    for (const std::string& arg : commandArgs) {
        if (arg.find("--output-format") != std::string::npos) continue;
        cmd << " " << arg;
    }

    // Use stream-json for streaming output
    cmd << " --output-format stream-json";

    // Session management
    if (!currentSessionId.empty()) {
        cmd << " --resume " << currentSessionId;
        fprintf(stderr, "DEBUG: [streaming] Using session: %s\n", currentSessionId.c_str());
    }

    // System prompt: prefer a file if we can find one (works from repo root or build/app).
    const std::string promptFilePath = ww_find_first_existing_upwards({
        "wibandwob.prompt.md",
        "app/wibandwob.prompt.md",
        "modules-private/wibwob-prompts/wibandwob.prompt.md",
        "modules/wibwob-prompts/wibandwob.prompt.md",
        "test-tui/wibandwob.prompt.md",
        "app/test-tui/wibandwob.prompt.md",
    }, 6);

    if (!promptFilePath.empty()) {
        cmd << " --system-prompt-file " << promptFilePath;
    } else if (!systemPrompt.empty()) {
        cmd << " --append-system-prompt \"";
        for (char c : systemPrompt) {
            if (c == '"' || c == '\\' || c == '$' || c == '`') {
                cmd << '\\';
            }
            cmd << c;
        }
        cmd << "\"";
    }

    // Escape the query
    cmd << " \"";
    for (char c : query) {
        if (c == '"' || c == '\\' || c == '$' || c == '`') {
            cmd << '\\';
        }
        cmd << c;
    }
    cmd << "\" 2>&1";

    fprintf(stderr, "DEBUG: [streaming] Command: %s\n", cmd.str().c_str());

    // Start async execution
    activePipe = popen(cmd.str().c_str(), "r");
    if (!activePipe) {
        streamingMode = false;
        streamingActive = false;
        StreamChunk chunk;
        chunk.type = StreamChunk::ERROR_OCCURRED;
        chunk.error_message = "Failed to execute streaming command";
        if (activeStreamCallback) activeStreamCallback(chunk);
        return false;
    }

    // Set non-blocking
    int fd = fileno(activePipe);
    int flags = fcntl(fd, F_GETFL, 0);
    fcntl(fd, F_SETFL, flags | O_NONBLOCK);

    busy = true;
    outputBuffer.clear();

    return true;
}
```

### `app/llm/providers/claude_code_sdk_provider.cpp` — SDK bridge script path resolution

```cpp
bool ClaudeCodeSDKProvider::isAvailable() const {
    // Check if Node.js is available
    int result = system("which node > /dev/null 2>&1");
    if (result != 0) {
        fprintf(stderr, "DEBUG: Node.js not available\n");
        return false;
    }
    
    // Check if bridge script exists (search upward so this works from repo root or build/app).
    const std::string resolvedScriptPath = ww_find_first_existing_upwards({nodeScriptPath}, 6);
    if (resolvedScriptPath.empty()) {
        fprintf(stderr, "DEBUG: Bridge script not found (searched upwards). Configured path: %s\n", nodeScriptPath.c_str());
        return false;
    }
    
    fprintf(stderr, "DEBUG: claude_code_sdk provider is available\n");
    return true;
}
```

```cpp
bool ClaudeCodeSDKProvider::configure(const std::string& config) {
    // Parse configuration (tolerant JSON-ish parsing without throwing)
    fprintf(stderr, "DEBUG: SDK Provider configure() called with: %s\n", config.c_str());

    auto parseIntField = [&config](const std::string& key, int defaultValue) -> int {
        size_t keyPos = config.find(key);
        if (keyPos == std::string::npos) return defaultValue;
        size_t colonPos = config.find(":", keyPos);
        if (colonPos == std::string::npos) return defaultValue;

        // Skip spaces
        size_t valPos = colonPos + 1;
        while (valPos < config.size() && (config[valPos] == ' ' || config[valPos] == '\t')) {
            ++valPos;
        }

        // Extract numeric text, supporting optional quotes
        std::string numStr;
        if (valPos < config.size() && config[valPos] == '"') {
            // Quoted number: "123"
            ++valPos;
            size_t endQuote = config.find('"', valPos);
            if (endQuote == std::string::npos) return defaultValue;
            numStr = config.substr(valPos, endQuote - valPos);
        } else {
            // Unquoted: read digits/sign until delimiter
            size_t endPos = valPos;
            while (endPos < config.size()) {
                char c = config[endPos];
                if ((c >= '0' && c <= '9') || c == '-' || c == '+') {
                    ++endPos;
                } else {
                    break;
                }
            }
            if (endPos == valPos) return defaultValue;
            numStr = config.substr(valPos, endPos - valPos);
        }

        try {
            return std::stoi(numStr);
        } catch (...) {
            return defaultValue;
        }
    };

    auto parseStringField = [&config](const std::string& key, std::string defaultValue) -> std::string {
        size_t keyPos = config.find(key);
        if (keyPos == std::string::npos) return defaultValue;
        size_t colonPos = config.find(":", keyPos);
        if (colonPos == std::string::npos) return defaultValue;
        size_t startQuote = config.find('"', colonPos);
        if (startQuote == std::string::npos) return defaultValue;
        ++startQuote;
        size_t endQuote = config.find('"', startQuote);
        if (endQuote == std::string::npos) return defaultValue;
        return config.substr(startQuote, endQuote - startQuote);
    };

    // maxTurns (quoted or numeric)
    maxTurns = parseIntField("maxTurns", maxTurns);

    // nodeScriptPath (resolve relative path by searching upward from CWD).
    nodeScriptPath = parseStringField("nodeScriptPath", nodeScriptPath);
    {
        const std::string resolved = ww_find_first_existing_upwards({nodeScriptPath}, 6);
        if (!resolved.empty()) {
            nodeScriptPath = resolved;
        }
    }

    // sessionTimeout (quoted or numeric)
    sessionTimeout = parseIntField("sessionTimeout", sessionTimeout);

    // model - map to full 4.5 IDs
    std::string modelStr = parseStringField("model", "claude-haiku-4-5");
    if (modelStr.find("opus") != std::string::npos) {
        configuredModel = "claude-opus-4-5";
    } else if (modelStr.find("sonnet") != std::string::npos) {
        configuredModel = "claude-sonnet-4-5";
    } else {
        configuredModel = "claude-haiku-4-5";  // Default
    }
    fprintf(stderr, "[SDK] Configured model: %s (from %s)\n", configuredModel.c_str(), modelStr.c_str());

    // allowedTools: if present, keep defaults for now
    allowedTools.clear();
    if (config.find("allowedTools") != std::string::npos) {
        allowedTools = {"Read", "Write", "Grep", "Bash", "LS", "WebSearch", "WebFetch"};
    }

    return true;
}
```

### `app/llm/providers/anthropic_api_provider.cpp` — non-blocking async + poll completion (fixes freeze)

```cpp
bool AnthropicAPIProvider::sendQuery(const LLMRequest& request, ResponseCallback callback) {
    if (busy || request.message.empty()) {
        return false;
    }
    
    clearError();
    busy = true;

    if (!isAvailable()) {
        busy = false;
        LLMResponse response;
        response.provider_name = getProviderName();
        response.model_used = model;
        response.is_error = true;
        response.error_message = "API key not configured";
        if (callback) callback(response);
        return false;
    }

    // Build JSON request payload.
    const std::string jsonRequest = buildSimpleRequestJson(request);

    // Create a unique temp file for payload (avoid collisions across concurrent runs).
    char tmpTemplate[] = "/tmp/anthropic_simple_XXXXXX";
    int tmpFd = mkstemp(tmpTemplate);
    if (tmpFd == -1) {
        busy = false;
        LLMResponse response;
        response.provider_name = getProviderName();
        response.model_used = model;
        response.is_error = true;
        response.error_message = "Failed to create temp file for payload";
        setError(response.error_message);
        if (callback) callback(response);
        return false;
    }

    activeTempFile = tmpTemplate;
    ssize_t written = write(tmpFd, jsonRequest.data(), jsonRequest.size());
    close(tmpFd);
    if (written < 0 || static_cast<size_t>(written) != jsonRequest.size()) {
        unlink(activeTempFile.c_str());
        activeTempFile.clear();
        busy = false;
        LLMResponse response;
        response.provider_name = getProviderName();
        response.model_used = model;
        response.is_error = true;
        response.error_message = "Failed to write payload to temp file";
        setError(response.error_message);
        if (callback) callback(response);
        return false;
    }

    // Build curl command.
    std::string curlCmd = "curl -sS --max-time 30 ";
    curlCmd += "-H \"Content-Type: application/json\" ";
    curlCmd += "-H \"x-api-key: " + apiKey + "\" ";
    curlCmd += "-H \"anthropic-version: 2023-06-01\" ";
    curlCmd += "-X POST ";
    curlCmd += "\"" + endpoint + "\" ";
    curlCmd += "--data @" + activeTempFile;

    // Log without leaking the API key.
    const std::string maskedKey = apiKey.size() > 16 ? apiKey.substr(0, 16) + "****" : "****";
    fprintf(stderr, "DEBUG: Anthropic API call (async): POST %s (key: %s, payload: %s)\n",
            endpoint.c_str(), maskedKey.c_str(), activeTempFile.c_str());

    // Start async execution (non-blocking read in poll()).
    activePipe = popen(curlCmd.c_str(), "r");
    if (!activePipe) {
        unlink(activeTempFile.c_str());
        activeTempFile.clear();
        busy = false;
        LLMResponse response;
        response.provider_name = getProviderName();
        response.model_used = model;
        response.is_error = true;
        response.error_message = "Failed to execute curl";
        setError(response.error_message);
        if (callback) callback(response);
        return false;
    }

    // Set non-blocking mode on pipe.
    int fd = fileno(activePipe);
    int flags = fcntl(fd, F_GETFL, 0);
    fcntl(fd, F_SETFL, flags | O_NONBLOCK);

    pendingCallback = callback;
    pendingRequest = request;
    outputBuffer.clear();
    requestStart = std::chrono::high_resolution_clock::now();

    return true;
}
```

```cpp
void AnthropicAPIProvider::cancel() {
    if (!busy) return;

    busy = false;

    if (activePipe) {
        pclose(activePipe);
        activePipe = nullptr;
    }

    if (!activeTempFile.empty()) {
        unlink(activeTempFile.c_str());
        activeTempFile.clear();
    }

    outputBuffer.clear();

    if (pendingCallback) {
        LLMResponse response;
        response.provider_name = getProviderName();
        response.model_used = model;
        response.is_error = true;
        response.error_message = "Request cancelled by user";
        pendingCallback(response);
        pendingCallback = nullptr;
    }
}
```

```cpp
void AnthropicAPIProvider::poll() {
    if (!busy || !activePipe) {
        return;
    }

    // Read available output (non-blocking).
    char buffer[4096];
    clearerr(activePipe);
    size_t bytesRead = fread(buffer, 1, sizeof(buffer) - 1, activePipe);
    if (bytesRead > 0) {
        buffer[bytesRead] = '\0';
        outputBuffer += buffer;
    }

    // Check for completion.
    if (feof(activePipe)) {
        int exitCode = pclose(activePipe);
        activePipe = nullptr;
        busy = false;

        auto endTime = std::chrono::high_resolution_clock::now();
        const int durationMs = (int)std::chrono::duration_cast<std::chrono::milliseconds>(
            endTime - requestStart).count();

        LLMResponse response;
        response.provider_name = getProviderName();
        response.model_used = model;
        response.duration_ms = durationMs;

        if (exitCode != 0 || outputBuffer.empty()) {
            response.is_error = true;
            response.error_message = "Curl failed or empty response. Exit code: " + std::to_string(exitCode);
            response.session_id = "CURL_ERROR: " + outputBuffer;
            setError(response.error_message);
        } else {
            response = parseSimpleResponse(outputBuffer);
            response.provider_name = getProviderName();
            response.model_used = model;
            response.duration_ms = durationMs;

            // Add tool count debug info (parity with previous synchronous path).
            std::vector<Tool> allTools = registeredTools;
            allTools.insert(allTools.end(), pendingRequest.tools.begin(), pendingRequest.tools.end());
            response.session_id += " TOOLS_SENT: " + std::to_string(allTools.size());

            // Add to conversation history if successful.
            if (!response.is_error) {
                if (!pendingRequest.message.empty()) {
                    conversationHistory.push_back(std::make_pair("user", pendingRequest.message));
                }
                if (!response.result.empty()) {
                    conversationHistory.push_back(std::make_pair("assistant", response.result));
                }
            }
        }

        if (!activeTempFile.empty()) {
            unlink(activeTempFile.c_str());
            activeTempFile.clear();
        }

        outputBuffer.clear();
        pendingRequest = LLMRequest{};

        if (pendingCallback) {
            pendingCallback(response);
            pendingCallback = nullptr;
        }
    }
}
```

If you want, I can also add a tiny `wibandwob.prompt.md` template under `app/` (or repo root) so the startup message stops appearing in a clean checkout.
